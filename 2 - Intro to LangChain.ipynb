{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Langchain Components Demo\n",
    "\n",
    "The full documentation on Langchain can be found at:\n",
    "https://python.langchain.com/v0.1/docs/get_started/quickstart/"
   ],
   "id": "2ed754d49a62fa66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting Started\n",
    "Install the packages with pip.\n",
    "1. Download and install [Anaconda Navigator](https://www.anaconda.com/download/)\n",
    "2. Launch Jupyter Notebook from Anaconda Navigator\n",
    "3. Go to \"New\" -> \"Terminal\"\n",
    "4. Run the following command in the terminal\n",
    "5. Open this `demo.ipynb` in Jupyter Notebook\n",
    "\n",
    "```bash\n",
    "pip install -U langchain==0.2.0\n",
    "pip install -U langchain-core==0.2.0\n",
    "pip install -U langchain-community==0.2.0\n",
    "pip install -U langchain-openai==0.1.7\n",
    "pip install -U langchain-experimental==0.0.59\n",
    "pip install -U wikipedia\n",
    "pip install -U yfinance\n",
    "pip install -U tabulate\n",
    "pip install python-dotenv\n",
    "pip install langchainhub\n",
    "```"
   ],
   "id": "65b9231a6d39abc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prompt Template\n",
    "Prompt templates are **predefined recipes for generating prompts** for language models such as OpenAI's GPT model.\n",
    "\n",
    "A template may include:\n",
    "- instructions, \n",
    "- few-shot examples, and \n",
    "- specific context and questions appropriate for a given task.\n",
    "\n",
    "Documentation: [Prompt Templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/)"
   ],
   "id": "c1e71fc8e5cebaad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 PromptTemplate\n",
    "Use PromptTemplate to create a template for a string prompt."
   ],
   "id": "1bfac7a3ca806f8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ],
   "id": "3db8f55e5ce9123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 PromptTemplate with `input variables` and `partial variables`\n",
    "\n",
    "Like other methods, it can make sense to \"partial\" a prompt template - e.g. `pass in a subset of the required values`, as to create a new prompt template which expects only the remaining subset of values.\n",
    "\n"
   ],
   "id": "dd89b2d17ae8fe66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    input_variables=[\"adjective\", \"content\"],\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ],
   "id": "637102b22bb49c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"adjective\": \"funny\"},\n",
    ")\n",
    "prompt_template.format(adjective=\"hilarious\", content=\"chickens\")"
   ],
   "id": "4621ef581d7dba52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 ChatPromptTemplate\n",
    "ChatPromptTemplate is a template to `chat models` and it contains a list of `chat messages` that can be used to interact with the model.\n",
    "Each chat message is associated with content, and an additional parameter called `role`. \n",
    "\n",
    "For example, in the `OpenAI Chat Completions API`, a chat message can be associated with an `AI assistant`, a `human` or a `system` role.\n",
    "\n",
    "Let's create a chat template for `OpenAI chat model`."
   ],
   "id": "2ad82676755cc81b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ],
   "id": "2999de307839e7a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the chat model with the generated messages.",
   "id": "5abadc6b6b64b14f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI Chat model\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "model.invoke(messages)"
   ],
   "id": "c1ce5467ae6a30c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 MessagesPlaceholder\n",
    "`MessagesPlaceholder` gives you full control of what messages to be rendered during formatting. \n",
    "\n",
    "This can be useful when you are uncertain of:\n",
    "- what **role** you should be using for your message prompt templates or ;\n",
    "- when you wish to insert a **list of messages** during formatting\n",
    "\n",
    "Let's create a simple chat prompt using `MessagesPlaceholder`:"
   ],
   "id": "25d19a152129358f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"some_conversation\"), human_message_template]\n",
    ")\n",
    "chat_prompt"
   ],
   "id": "ea4bc5d4c2a0970a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's format the `chat_prompt` with some conversation messages.",
   "id": "a4ba217accc0efa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Format the chat prompt by passing the conversation messages\n",
    "messages = chat_prompt.format_prompt(\n",
    "    some_conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()\n",
    "\n",
    "messages\n"
   ],
   "id": "a0fef88962c2a082",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the Open AI chat model with the generated `messages`.",
   "id": "dba238403b04dac6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.invoke(messages)",
   "id": "3f0c32754f354eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Output Parsers\n",
    "Output parsers are responsible for taking the output of an LLM and **transforming it to a more suitable format** e.g `.json`, `.csv`. \n",
    "\n",
    "This is very useful when you are using LLMs to generate any form of structured data.\n",
    "\n",
    "Documentation: [Output Parsers](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/)"
   ],
   "id": "74cbf8058e69a261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Simple JsonOutputParser\n",
    "This is a simple output parser that converts the output of an LLM to a JSON object."
   ],
   "id": "b1d346c885f874a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Simple Json Output Parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Get format instructions for the prompt template.\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Define a prompt template with the parser's format instructions.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "prompt\n"
   ],
   "id": "e95b44c05b7e80a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the model with the `prompt` and `user_query`.",
   "id": "d09b71015459c77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Initialize the OpenAI Chat model\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# A query intended to prompt a language model to populate the data structure.\n",
    "user_query = \"What car has a very good horse power?\"\n",
    "\n",
    "# Invoke the model with the prompt\n",
    "# model.invoke(prompt.format(query=user_query))\n",
    "\n",
    "content = model.invoke(prompt.format(query=user_query)).content\n",
    "\n",
    "json_obj = json.loads(content)\n",
    "json_obj\n"
   ],
   "id": "22adca20fea61f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 JsonOutputParser with Pydantic\n",
    "This output parser allows users to specify an **arbitrary JSON schema** and query LLMs for outputs that **conform to that schema**.\n"
   ],
   "id": "a3f948c8187e26a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure for JSON.\n",
    "class Car(BaseModel):\n",
    "    brand: str = Field(description=\"the brand of the car in string\")\n",
    "    model: str = Field(description=\"the model of the car in string\")\n",
    "    hp: int = Field(description=\"the horse power of the car in integer\")\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Car)\n",
    "\n",
    "# Get format instructions for the prompt template.\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Define a prompt template with the parser's format instructions.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "prompt"
   ],
   "id": "11a964c8a68e6dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the model with the `prompt` and `user_query`.",
   "id": "60afcb370585e4b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A query intended to prompt a language model to populate the data structure.\n",
    "user_query = \"What car has a very good horse power?\"\n",
    "\n",
    "# Invoke the model with the prompt\n",
    "# model.invoke(prompt.format(query=user_query))\n",
    "\n",
    "content = model.invoke(\n",
    "    prompt.format(query=user_query)\n",
    ").content\n",
    "\n",
    "json_obj = json.loads(content)\n",
    "json_obj"
   ],
   "id": "4ec2308409695381",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 CSVOutputParser (Exercise)\n",
    "Can you create a prompt template that output a `CSV` list of 5 `{subject}`?\n",
    "\n",
    "E.g. List 5 `car brands`; List 5 `programming languages`;\n",
    "\n",
    "Reference: [CommaSeparatedListOutputParser](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/csv/)\n"
   ],
   "id": "7fb9062cb0521d6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Your code here"
   ],
   "id": "e8122c2dae390771",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Chaining with LLMChain & LangChain Expression Language (LCEL)\n",
    "\n",
    "A **Chain** refers to a sequence of steps or operations that are executed in a specific order to accomplish a task using LLMs and other computational components. \n",
    "\n",
    "Chains are a core concept in LangChain, allowing developers to build complex workflows that involve multiple interactions with:\n",
    "- prompts,\n",
    "- models,\n",
    "- arbitrary functions,\n",
    "- or even other chains\n",
    "\n",
    "The primary supported way to do this is with `LCEL`.\n",
    "\n",
    "Documentation: [Chaining](https://python.langchain.com/v0.1/docs/modules/chains/)\n",
    "\n"
   ],
   "id": "d1bdc67c4fc6296c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Simple Chaining with LLMChain\n",
   "id": "9fedf805a0f112d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"List 5 {subject}.\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=model, \n",
    "    prompt=prompt,\n",
    "    output_parser=parser,\n",
    ")\n",
    "\n",
    "chain.invoke({\"subject\": subject})"
   ],
   "id": "dc3f23d26437505e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Simple Chaining with LCEL\n",
    "We can use LangChain runnables to chain together prompts, models, and parsers.\n",
    "\n",
    "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\n",
    "\n",
    "LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n",
    "\n",
    "Documentation: [LCEL](https://python.langchain.com/v0.1/docs/expression_language/)"
   ],
   "id": "777b05b2b5cdb0d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chaining different components using LCEL pipe operator\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"subject\":subject})"
   ],
   "id": "ba8f93c11cdaf955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Combined Chaining with LCEL (Sequential)\n",
    "We can even combine this chain with more runnables to create another chain."
   ],
   "id": "86f610fc4ca53849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a second prompt template\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"How do you classify these brands in terms of quality and pricing? {brands}\")\n",
    "\n",
    "combined_chain = {\"brands\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "combined_chain.invoke({\"subject\": subject})\n"
   ],
   "id": "1afd485ca26b8763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Parrallel Chaining with LCEL\n",
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map."
   ],
   "id": "12acf35ed2ee498b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "poem_chain = poem_prompt | model | StrOutputParser()\n",
    "\n",
    "map_chain = RunnableParallel(\n",
    "    joke=joke_chain, \n",
    "    poem=poem_chain\n",
    ")\n",
    "map_chain.invoke({\"topic\": \"cat\"})\n"
   ],
   "id": "7a6ccbfb4e586cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Using custom functions within Chain\n",
    "You can also use arbitrary functions in the pipeline."
   ],
   "id": "19739df03d27cf68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def get_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def get_multiple_length(_dict):\n",
    "    return len(_dict[\"text1\"]) * len(_dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(get_length),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}| RunnableLambda(get_multiple_length),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"hi\", \"bar\": \"world\"})"
   ],
   "id": "eb5c50352ad0f46c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Memory\n",
    "Most LLM applications have a conversational interface. \n",
    "\n",
    "An essential component of a conversation is being able to refer to information introduced earlier in the conversation. \n",
    "\n",
    "At bare minimum, a conversational system should be able to access some window of past messages directly. \n",
    "\n",
    "Documentation: [Memory](https://python.langchain.com/v0.1/docs/modules/memory/)"
   ],
   "id": "1e289bd83ceba378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 In-Memory with LCEL\n",
    "The `RunnableWithMessageHistory` lets us add message history to certain types of chains. It wraps another `Runnable` and manages the chat message history for it.\n",
    "\n",
    "Below we show a simple example in which the chat history lives in memory, in this case via a global Python dict.\n",
    "\n",
    "Documentation: [Memory LCEL](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/)"
   ],
   "id": "cd0ac2029a7355f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | model"
   ],
   "id": "4e19f1e9da393b0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # Create a new chat message history\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    # the key to be treated as the latest input message\n",
    "    input_messages_key=\"input\",\n",
    "    # the key to add historical messages to\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ],
   "id": "b2b5fb06f1a12d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ],
   "id": "7ea0e260439a6804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recalling the last message",
   "id": "cc9ca07d4f1cfd2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ],
   "id": "faa9ccbf4c438045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What if there isn't a message history to refer to?",
   "id": "45cd012d0bd3269f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ],
   "id": "f03d46a3db427c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Tools and Toolkits\n",
    "Tools are interfaces that an agent, chain, or LLM can use to interact with the world. They combine a few things:\n",
    "\n",
    "- The name of the tool\n",
    "- A description of what the tool is\n",
    "- JSON schema of what the inputs to the tool are\n",
    "- The function to call\n",
    "- Whether the result of a tool should be returned directly to the user\n",
    "\n",
    "It is useful to have all this information because this information can be used to build action-taking systems! The name, description, and JSON schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action.\n",
    "\n",
    "Documentation: [Toolkits](https://python.langchain.com/v0.1/docs/modules/tools/)"
   ],
   "id": "749fc99c43d61e07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1 Default Tools (Wikipedia Query Tool)\n",
    "The Wikipedia Query tool allows you to query Wikipedia for information.\n"
   ],
   "id": "41eceb9a29d68ad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ],
   "id": "d54f21ad6cc9cd33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Important information for LLM to recoginise and make use of the tool.",
   "id": "1b4eb9e4c2c04aae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tool.name",
   "id": "6f35e9066ccbdfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tool.description",
   "id": "fd09d5513384959f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tool.args",
   "id": "85ad853d313b7b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's run the tool with a query!",
   "id": "ed0724f05e8e0896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tool.run({\"query\": \"langchain\"})",
   "id": "ea67028f5f29fe3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Yahoo Finance News Tool (Exercise)\n",
    "The Yahoo Finance News tool allows you to query Yahoo Finance for news articles.\n",
    "\n",
    "Hint: use `tool.name`, `tool.description`, and `tool.args` to understand the tool."
   ],
   "id": "6c03c914e9437d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.tools import YahooFinanceNewsTool\n",
    "\n",
    "# Your code here"
   ],
   "id": "3e4a605d5ec7549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3 Toolkits\n",
    "Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods. \n",
    "\n",
    "For a complete list of available ready-made toolkits, visit [Integrations](https://python.langchain.com/v0.1/docs/integrations/toolkits/).\n",
    "\n",
    "We will demonstrate this using `Agents`."
   ],
   "id": "427665ff9ee2f9df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Agents\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. \n",
    "\n",
    "In chains, a sequence of actions is **hardcoded** (in code). \n",
    "\n",
    "In agents, a language model is used as a **reasoning engine** to determine which actions to take and in which order.\n",
    "\n",
    "Documentation: [Agents](https://python.langchain.com/v0.2/docs/integrations/toolkits/)"
   ],
   "id": "2e4131836a2fe18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.1 Agent with CSV Tooklits\n",
    "This notebook shows how to use `agents` to interact with data in `CSV format`. \n",
    "\n",
    "It is mostly optimized for question answering."
   ],
   "id": "149545c7701f1cdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "\n",
    "# A zero shot agent that does a reasoning step before acting.\n",
    "agent = create_csv_agent(\n",
    "    model,\n",
    "    \"titanic.csv\",\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent.invoke(\"What is the average age of the passengers?\")"
   ],
   "id": "15c04a6e0bfe53cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alternative `agent_type`:",
   "id": "b452a1ad453c093d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# An agent optimized for using open AI functions.\n",
    "agent = create_csv_agent(\n",
    "    model,\n",
    "    \"titanic.csv\",\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True\n",
    ")\n",
    "agent.invoke(\"What is the average age of the passengers?\")"
   ],
   "id": "bdf25d23e2e37175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "agent.invoke(\"how many people have more than 3 siblings\")",
   "id": "e53571dc92b851bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.2 Agent with Python Toolkit\n",
    "We can design the agent to write and execute Python code to answer a question."
   ],
   "id": "79e99d54c04e35aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "# Pulling a predefined prompt template from the Langchain hub. \n",
    "# This template is designed for OpenAI's language models\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is the 10th fibonacci number?\"})\n"
   ],
   "id": "eb3bdd1484586155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Huggingingface Models\n",
    "The [Hugging Face](https://huggingface.co/docs/hub/index) Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "Documentation: [Huggingface Endpoints](https://python.langchain.com/v0.1/docs/integrations/llms/huggingface_endpoint/)\n"
   ],
   "id": "29d30a03d6c6690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "70fc3b62d3879ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
