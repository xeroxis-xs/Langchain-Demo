{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Langchain Components Demo\n",
    "This demo can be downloaded from:\n",
    "\n",
    "The full documentation on Langchain can be found at:\n",
    "https://python.langchain.com/v0.1/docs/get_started/quickstart/"
   ],
   "id": "2ed754d49a62fa66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting Started\n",
    "Install the packages with pip.\n",
    "1. Download and install [Anaconda Navigator](https://www.anaconda.com/download/)\n",
    "2. Launch Jupyter Notebook from Anaconda Navigator\n",
    "3. Go to \"New\" -> \"Terminal\"\n",
    "4. Run the following command in the terminal\n",
    "5. Open this `demo.ipynb` in Jupyter Notebook\n",
    "\n",
    "```bash\n",
    "pip install -U langchain==0.2.0\n",
    "pip install -U langchain-core==0.2.0\n",
    "pip install -U langchain-community==0.2.0\n",
    "pip install -U langchain-openai==0.1.7\n",
    "pip install -U langchain-experimental==0.0.59\n",
    "pip install -U wikipedia\n",
    "pip install -U yfinance\n",
    "pip install -U tabulate\n",
    "```"
   ],
   "id": "65b9231a6d39abc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:22.762121Z",
     "start_time": "2024-05-19T13:08:22.759811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting API Key and environment variables\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"685158aec57747dfad586110be180657\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://anv2dev-azureopenai-auseast.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"anv2dev-gpt35turbo1106-auseast\""
   ],
   "id": "104de71d7bc3efbc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prompt Template\n",
    "Prompt templates are **predefined recipes for generating prompts** for language models such as OpenAI's GPT model.\n",
    "\n",
    "A template may include:\n",
    "- instructions, \n",
    "- few-shot examples, and \n",
    "- specific context and questions appropriate for a given task.\n",
    "\n",
    "Documentation: [Prompt Templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/)\n",
    "\n"
   ],
   "id": "c1e71fc8e5cebaad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 PromptTemplate\n",
    "Use PromptTemplate to create a template for a string prompt."
   ],
   "id": "1bfac7a3ca806f8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:23.226085Z",
     "start_time": "2024-05-19T13:08:22.993669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ],
   "id": "3db8f55e5ce9123",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 PromptTemplate with `input variables` and `partial variables`\n",
    "\n",
    "Like other methods, it can make sense to \"partial\" a prompt template - e.g. `pass in a subset of the required values`, as to create a new prompt template which expects only the remaining subset of values.\n",
    "\n"
   ],
   "id": "dd89b2d17ae8fe66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:23.229281Z",
     "start_time": "2024-05-19T13:08:23.226984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    input_variables=[\"adjective\", \"content\"],\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ],
   "id": "637102b22bb49c97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:23.231936Z",
     "start_time": "2024-05-19T13:08:23.229868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"adjective\": \"funny\"},\n",
    ")\n",
    "prompt_template.format(adjective=\"hilarious\", content=\"chickens\")"
   ],
   "id": "4621ef581d7dba52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a hilarious joke about chickens.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 ChatPromptTemplate\n",
    "ChatPromptTemplate is a template to `chat models` and it contains a list of `chat messages` that can be used to interact with the model.\n",
    "Each chat message is associated with content, and an additional parameter called `role`. \n",
    "\n",
    "For example, in the `OpenAI Chat Completions API`, a chat message can be associated with an `AI assistant`, a `human` or a `system` role.\n",
    "\n",
    "Let's create a chat template for `OpenAI chat model`."
   ],
   "id": "2ad82676755cc81b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:23.235435Z",
     "start_time": "2024-05-19T13:08:23.233006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ],
   "id": "2999de307839e7a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the chat model with the generated messages.",
   "id": "5abadc6b6b64b14f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:24.277004Z",
     "start_time": "2024-05-19T13:08:23.235969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI Chat model\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "model.invoke(messages)"
   ],
   "id": "c1ce5467ae6a30c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 50, 'total_tokens': 62}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-bc5b06fc-e655-40ac-82de-acb8d6168b55-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 MessagesPlaceholder\n",
    "`MessagesPlaceholder` gives you full control of what messages to be rendered during formatting. \n",
    "\n",
    "This can be useful when you are uncertain of:\n",
    "- what **role** you should be using for your message prompt templates or ;\n",
    "- when you wish to insert a **list of messages** during formatting\n",
    "\n",
    "Let's create a simple chat prompt using `MessagesPlaceholder`:"
   ],
   "id": "25d19a152129358f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:24.283588Z",
     "start_time": "2024-05-19T13:08:24.279497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"some_conversation\"), human_message_template]\n",
    ")\n",
    "chat_prompt"
   ],
   "id": "ea4bc5d4c2a0970a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['some_conversation', 'word_count'], input_types={'some_conversation': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[MessagesPlaceholder(variable_name='some_conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], template='Summarize our conversation so far in {word_count} words.'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's format the `chat_prompt` with some conversation messages.",
   "id": "a4ba217accc0efa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:24.287872Z",
     "start_time": "2024-05-19T13:08:24.284454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Format the chat prompt by passing the conversation messages\n",
    "messages = chat_prompt.format_prompt(\n",
    "    some_conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()\n",
    "\n",
    "messages\n"
   ],
   "id": "a0fef88962c2a082",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?'),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the Open AI chat model with the generated `messages`.",
   "id": "dba238403b04dac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:24.686399Z",
     "start_time": "2024-05-19T13:08:24.288771Z"
    }
   },
   "cell_type": "code",
   "source": "model.invoke(messages)",
   "id": "3f0c32754f354eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Learning programming: choose language, start with basics, practice consistently.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 99, 'total_tokens': 112}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-5278d68d-2d9d-4a2b-9603-9a8c6cf88f4a-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Output Parsers\n",
    "Output parsers are responsible for taking the output of an LLM and **transforming it to a more suitable format** e.g `.json`, `.csv`. \n",
    "\n",
    "This is very useful when you are using LLMs to generate any form of structured data.\n",
    "\n",
    "Documentation: [Output Parsers](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/)"
   ],
   "id": "74cbf8058e69a261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Simple JsonOutputParser\n",
    "This is a simple output parser that converts the output of an LLM to a JSON object."
   ],
   "id": "b1d346c885f874a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:24.694328Z",
     "start_time": "2024-05-19T13:08:24.688635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Simple Json Output Parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Get format instructions for the prompt template.\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Define a prompt template with the parser's format instructions.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "prompt\n"
   ],
   "id": "e95b44c05b7e80a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the model with the `prompt` and `user_query`.",
   "id": "d09b71015459c77d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:25.505743Z",
     "start_time": "2024-05-19T13:08:24.698440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Initialize the OpenAI Chat model\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# A query intended to prompt a language model to populate the data structure.\n",
    "user_query = \"What car has a very good horse power?\"\n",
    "\n",
    "# Invoke the model with the prompt\n",
    "# model.invoke(prompt.format(query=user_query))\n",
    "\n",
    "content = model.invoke(prompt.format(query=user_query)).content\n",
    "\n",
    "json_obj = json.loads(content)\n",
    "json_obj\n"
   ],
   "id": "22adca20fea61f47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'car': 'Tesla Model S', 'horsepower': '762'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 JsonOutputParser with Pydantic\n",
    "This output parser allows users to specify an **arbitrary JSON schema** and query LLMs for outputs that **conform to that schema**.\n"
   ],
   "id": "a3f948c8187e26a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:25.517822Z",
     "start_time": "2024-05-19T13:08:25.507762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure for JSON.\n",
    "class Car(BaseModel):\n",
    "    brand: str = Field(description=\"the brand of the car in string\")\n",
    "    model: str = Field(description=\"the model of the car in string\")\n",
    "    hp: int = Field(description=\"the horse power of the car in integer\")\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Car)\n",
    "\n",
    "# Get format instructions for the prompt template.\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Define a prompt template with the parser's format instructions.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "prompt"
   ],
   "id": "11a964c8a68e6dbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"brand\": {\"title\": \"Brand\", \"description\": \"the brand of the car in string\", \"type\": \"string\"}, \"model\": {\"title\": \"Model\", \"description\": \"the model of the car in string\", \"type\": \"string\"}, \"hp\": {\"title\": \"Hp\", \"description\": \"the horse power of the car in integer\", \"type\": \"integer\"}}, \"required\": [\"brand\", \"model\", \"hp\"]}\\n```'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's invoke the model with the `prompt` and `user_query`.",
   "id": "60afcb370585e4b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:26.015549Z",
     "start_time": "2024-05-19T13:08:25.519083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A query intended to prompt a language model to populate the data structure.\n",
    "user_query = \"What car has a very good horse power?\"\n",
    "\n",
    "# Invoke the model with the prompt\n",
    "# model.invoke(prompt.format(query=user_query))\n",
    "\n",
    "content = model.invoke(\n",
    "    prompt.format(query=user_query)\n",
    ").content\n",
    "\n",
    "json_obj = json.loads(content)\n",
    "json_obj"
   ],
   "id": "4ec2308409695381",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand': 'Tesla', 'model': 'Model S', 'hp': 503}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 CSVOutputParser (Exercise)\n",
    "Can you create a prompt template that output a `CSV` list of 5 `{subject}`?\n",
    "\n",
    "E.g. List 5 `car brands`; List 5 `programming languages`;\n",
    "\n",
    "Reference: [CommaSeparatedListOutputParser](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/csv/)\n"
   ],
   "id": "7fb9062cb0521d6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:26.315112Z",
     "start_time": "2024-05-19T13:08:26.017272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Get format instructions for the prompt template.\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Define a prompt template with the parser's format instructions.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 5 {subject}\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "subject = \"car brands\"\n",
    "\n",
    "content = model.invoke(\n",
    "    prompt.format(subject=subject)\n",
    ").content\n",
    "\n",
    "content"
   ],
   "id": "e8122c2dae390771",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toyota, Ford, Honda, Chevrolet, BMW'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Chaining with LLMChain & LangChain Expression Language (LCEL)\n",
    "\n",
    "A **Chain** refers to a sequence of steps or operations that are executed in a specific order to accomplish a task using LLMs and other computational components. \n",
    "\n",
    "Chains are a core concept in LangChain, allowing developers to build complex workflows that involve multiple interactions with:\n",
    "- prompts,\n",
    "- models,\n",
    "- arbitrary functions,\n",
    "- or even other chains\n",
    "\n",
    "The primary supported way to do this is with `LCEL`.\n",
    "\n",
    "Documentation: [Chaining](https://python.langchain.com/v0.1/docs/modules/chains/)\n",
    "\n"
   ],
   "id": "d1bdc67c4fc6296c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Simple Chaining with LLMChain\n",
   "id": "9fedf805a0f112d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:26.773773Z",
     "start_time": "2024-05-19T13:08:26.316562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"List 5 {subject}.\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=model, \n",
    "    prompt=prompt,\n",
    "    output_parser=parser,\n",
    ")\n",
    "\n",
    "chain.invoke({\"subject\": subject})"
   ],
   "id": "dc3f23d26437505e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xeroxis/Documents/GitHub/Langchain-Demo/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject': 'car brands',\n",
       " 'text': '1. Toyota\\n2. Ford\\n3. BMW\\n4. Honda\\n5. Mercedes-Benz'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Simple Chaining with LCEL\n",
    "We can use LangChain runnables to chain together prompts, models, and parsers.\n",
    "\n",
    "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\n",
    "\n",
    "LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n",
    "\n",
    "Documentation: [LCEL](https://python.langchain.com/v0.1/docs/expression_language/)"
   ],
   "id": "777b05b2b5cdb0d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:27.250051Z",
     "start_time": "2024-05-19T13:08:26.775340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chaining different components using LCEL pipe operator\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"subject\":subject})"
   ],
   "id": "ba8f93c11cdaf955",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Toyota\\n2. Ford\\n3. BMW\\n4. Honda\\n5. Mercedes-Benz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Combined Chaining with LCEL (Sequential)\n",
    "We can even combine this chain with more runnables to create another chain."
   ],
   "id": "86f610fc4ca53849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:28.373380Z",
     "start_time": "2024-05-19T13:08:27.251876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a second prompt template\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"How do you classify these brands in terms of quality and pricing? {brands}\")\n",
    "\n",
    "combined_chain = {\"brands\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "combined_chain.invoke({\"subject\": subject})\n"
   ],
   "id": "1afd485ca26b8763",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Toyota - Mid-range quality and mid-range pricing\\n2. Ford - Mid-range quality and mid-range pricing\\n3. BMW - High-quality and high pricing\\n4. Honda - Mid-range quality and mid-range pricing\\n5. Mercedes-Benz - High-quality and high pricing'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Parrallel Chaining with LCEL\n",
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map."
   ],
   "id": "12acf35ed2ee498b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:29.160487Z",
     "start_time": "2024-05-19T13:08:28.379999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "poem_chain = poem_prompt | model | StrOutputParser()\n",
    "\n",
    "map_chain = RunnableParallel(\n",
    "    joke=joke_chain, \n",
    "    poem=poem_chain\n",
    ")\n",
    "map_chain.invoke({\"topic\": \"cat\"})\n"
   ],
   "id": "7a6ccbfb4e586cfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!',\n",
       " 'poem': 'Whiskers soft and eyes so bright,\\nPurring softly through the night.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Using custom functions within Chain\n",
    "You can also use arbitrary functions in the pipeline."
   ],
   "id": "19739df03d27cf68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:29.477330Z",
     "start_time": "2024-05-19T13:08:29.162164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def get_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def get_multiple_length(_dict):\n",
    "    return len(_dict[\"text1\"]) * len(_dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(get_length),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}| RunnableLambda(get_multiple_length),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"hi\", \"bar\": \"world\"})"
   ],
   "id": "eb5c50352ad0f46c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 10 equals 12.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Memory\n",
    "Most LLM applications have a conversational interface. \n",
    "\n",
    "An essential component of a conversation is being able to refer to information introduced earlier in the conversation. \n",
    "\n",
    "At bare minimum, a conversational system should be able to access some window of past messages directly. \n",
    "\n",
    "Documentation: [Memory](https://python.langchain.com/v0.1/docs/modules/memory/)"
   ],
   "id": "1e289bd83ceba378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 In-Memory with LCEL\n",
    "The `RunnableWithMessageHistory` lets us add message history to certain types of chains. It wraps another `Runnable` and manages the chat message history for it.\n",
    "\n",
    "Below we show a simple example in which the chat history lives in memory, in this case via a global Python dict.\n",
    "\n",
    "Documentation: [Memory LCEL](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/)"
   ],
   "id": "cd0ac2029a7355f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:29.482031Z",
     "start_time": "2024-05-19T13:08:29.478600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | model"
   ],
   "id": "4e19f1e9da393b0a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:29.492975Z",
     "start_time": "2024-05-19T13:08:29.483131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # Create a new chat message history\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    # the key to be treated as the latest input message\n",
    "    input_messages_key=\"input\",\n",
    "    # the key to add historical messages to\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ],
   "id": "b2b5fb06f1a12d70",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:30.424662Z",
     "start_time": "2024-05-19T13:08:29.493720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ],
   "id": "7ea0e260439a6804",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 33, 'total_tokens': 59}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-573739ef-8f1e-4063-bb84-4cd42a998b52-0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recalling the last message",
   "id": "cc9ca07d4f1cfd2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:30.828906Z",
     "start_time": "2024-05-19T13:08:30.425917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ],
   "id": "faa9ccbf4c438045",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a function that relates the angle of a right triangle to the ratio of its sides.', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 69, 'total_tokens': 89}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-b4614317-6a0e-45db-bef2-59ffa48e5552-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What if there isn't a message history to refer to?",
   "id": "45cd012d0bd3269f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:31.244394Z",
     "start_time": "2024-05-19T13:08:30.830665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ],
   "id": "f03d46a3db427c29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I can help with math problems. Just ask me a question and I'll do my best to assist you.\", response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 30, 'total_tokens': 52}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-9e7a8838-478c-4098-aef8-f16a721cf2e3-0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Tools and Toolkits\n",
    "Tools are interfaces that an agent, chain, or LLM can use to interact with the world. They combine a few things:\n",
    "\n",
    "- The name of the tool\n",
    "- A description of what the tool is\n",
    "- JSON schema of what the inputs to the tool are\n",
    "- The function to call\n",
    "- Whether the result of a tool should be returned directly to the user\n",
    "\n",
    "It is useful to have all this information because this information can be used to build action-taking systems! The name, description, and JSON schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action.\n",
    "\n",
    "Documentation: [Toolkits](https://python.langchain.com/v0.1/docs/modules/tools/)"
   ],
   "id": "749fc99c43d61e07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1 Default Tools (Wikipedia Query Tool)\n",
    "The Wikipedia Query tool allows you to query Wikipedia for information.\n"
   ],
   "id": "41eceb9a29d68ad1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:31.330166Z",
     "start_time": "2024-05-19T13:08:31.245416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ],
   "id": "d54f21ad6cc9cd33",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Important information for LLM to recoginise and make use of the tool.",
   "id": "1b4eb9e4c2c04aae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:31.332683Z",
     "start_time": "2024-05-19T13:08:31.330845Z"
    }
   },
   "cell_type": "code",
   "source": "tool.name",
   "id": "6f35e9066ccbdfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:31.334828Z",
     "start_time": "2024-05-19T13:08:31.333257Z"
    }
   },
   "cell_type": "code",
   "source": "tool.description",
   "id": "fd09d5513384959f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:31.338364Z",
     "start_time": "2024-05-19T13:08:31.335305Z"
    }
   },
   "cell_type": "code",
   "source": "tool.args",
   "id": "85ad853d313b7b9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's run the tool with a query!",
   "id": "ed0724f05e8e0896"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:08:32.442807Z",
     "start_time": "2024-05-19T13:08:31.341237Z"
    }
   },
   "cell_type": "code",
   "source": "tool.run({\"query\": \"langchain\"})",
   "id": "ea67028f5f29fe3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Yahoo Finance News Tool (Exercise)\n",
    "The Yahoo Finance News tool allows you to query Yahoo Finance for news articles.\n",
    "\n",
    "Hint: use `tool.name`, `tool.description`, and `tool.args` to understand the tool."
   ],
   "id": "6c03c914e9437d88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:16:11.024676Z",
     "start_time": "2024-05-19T13:16:05.665646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.tools import YahooFinanceNewsTool\n",
    "\n",
    "tool = YahooFinanceNewsTool()\n",
    "\n",
    "tool.run({\"query\": \"NFLX\"})\n"
   ],
   "id": "3e4a605d5ec7549",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 Must-Buy High-Flying U.S. Giants on Favorable Economic Data\\nWe have narrowed our search to five U.S. giants that have provided more than 20% returns year to date with more upside left. These are: NVDA, NFLX, PGR, GOOGL, WFC.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3 Toolkits\n",
    "Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods. \n",
    "\n",
    "For a complete list of available ready-made toolkits, visit [Integrations](https://python.langchain.com/v0.1/docs/integrations/toolkits/).\n",
    "\n",
    "We will demonstrate this using `Agents`."
   ],
   "id": "427665ff9ee2f9df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Agents\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. \n",
    "\n",
    "In chains, a sequence of actions is **hardcoded** (in code). \n",
    "\n",
    "In agents, a language model is used as a **reasoning engine** to determine which actions to take and in which order.\n",
    "\n",
    "Documentation: [Agents](https://python.langchain.com/v0.2/docs/integrations/toolkits/)"
   ],
   "id": "2e4131836a2fe18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.1 Agent with CSV Tooklits\n",
    "This notebook shows how to use `agents` to interact with data in `CSV format`. \n",
    "\n",
    "It is mostly optimized for question answering."
   ],
   "id": "149545c7701f1cdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:24:35.855864Z",
     "start_time": "2024-05-19T13:24:34.078428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "\n",
    "# A zero shot agent that does a reasoning step before acting.\n",
    "agent = create_csv_agent(\n",
    "    model,\n",
    "    \"titanic.csv\",\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent.invoke(\"What is the average age of the passengers?\")"
   ],
   "id": "15c04a6e0bfe53cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: I need to calculate the average of the 'Age' column in the dataframe.\n",
      "Action: python_repl_ast\n",
      "Action Input: df['Age'].mean()\u001B[0m\u001B[36;1m\u001B[1;3m29.69911764705882\u001B[0m\u001B[32;1m\u001B[1;3mThe average age of the passengers is 29.7 years.\n",
      "Final Answer: 29.7 years\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the average age of the passengers?', 'output': '29.7 years'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alternative `agent_type`:",
   "id": "b452a1ad453c093d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:25:47.376796Z",
     "start_time": "2024-05-19T13:25:46.156875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# An agent optimized for using open AI functions.\n",
    "agent = create_csv_agent(\n",
    "    model,\n",
    "    \"titanic.csv\",\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True\n",
    ")\n",
    "agent.invoke(\"What is the average age of the passengers?\")"
   ],
   "id": "bdf25d23e2e37175",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Age'].mean()\"}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m29.69911764705882\u001B[0m\u001B[32;1m\u001B[1;3mThe average age of the passengers is approximately 29.7 years.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the average age of the passengers?',\n",
       " 'output': 'The average age of the passengers is approximately 29.7 years.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:27:38.779315Z",
     "start_time": "2024-05-19T13:27:37.438618Z"
    }
   },
   "cell_type": "code",
   "source": "agent.run(\"how many people have more than 3 siblings\")",
   "id": "e53571dc92b851bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df[df['SibSp'] > 3]['PassengerId'].count()\"}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m30\u001B[0m\u001B[32;1m\u001B[1;3mThere are 30 people who have more than 3 siblings.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 30 people who have more than 3 siblings.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.2 Agent with Python Toolkit\n",
    "We can design the agent to write and execute Python code to answer a question."
   ],
   "id": "79e99d54c04e35aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T13:48:33.288793Z",
     "start_time": "2024-05-19T13:48:30.167881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is the 10th fibonacci number?\"})\n"
   ],
   "id": "eb3bdd1484586155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `Python_REPL` with `def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "fibonacci(10)`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m\u001B[0m\u001B[32;1m\u001B[1;3mThe 10th Fibonacci number is 55.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the 10th fibonacci number?',\n",
       " 'output': 'The 10th Fibonacci number is 55.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Huggingingface Models\n",
    "The [Hugging Face](https://huggingface.co/docs/hub/index) Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
   ],
   "id": "29d30a03d6c6690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70fc3b62d3879ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
